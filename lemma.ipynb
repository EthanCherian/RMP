{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, will probably want to use lemmatization (this notebook) over pure stemming (previous notebook).\n",
    "\n",
    "Primary difference is that stemming does not guarantee an inherently meaningful result, but lemmatization does.\n",
    "\n",
    "Same note as before: removing stopwords also removes words like \"not\" and \"don't\", obviously important for sentiment analysis so may require some ingenuity to get around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"omw-1.4\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"Good teacher, I took this as a second block class on accident. Lots of reading in the class, not super necessary for the lectures. \n",
    "            But a fun teacher who's excited to share English with students, normal amount of homework assignments and a small 5 page paper due at finals. 8/10\"\"\"\n",
    "# random review chosen\n",
    "\n",
    "sentences = nltk.sent_tokenize(review)          # break review up into component sentences\n",
    "lemmatizer = WordNetLemmatizer()                # set up lemmatizer\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting creative to keep negation words and phrases when removing stopwords\n",
    "realwords = stopwords.words(\"english\")\n",
    "realwords.remove(\"no\")\n",
    "realwords.remove(\"nor\")\n",
    "realwords.remove(\"not\")\n",
    "realwords.remove(\"didn't\")\n",
    "realwords.remove(\"don't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])            # tokenize entire sentence\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(realwords)]           # lemmatize every non-stopword in sentence\n",
    "    sentences[i] = \" \".join(words)                      # join back together\n",
    "\n",
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8ce2715666cca47edfe3dc40f16f58fc193b5a7b06958e2ae06c97afdf9990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
