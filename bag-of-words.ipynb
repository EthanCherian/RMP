{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mr Video Man said BoW works if we just want sentiment analysis, but falls off quickly when dataset is super large. With that said, we should be good on that front lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"\"\"Good teacher, I took this as a second block class on accident. Lots of reading in the class, not super necessary for the lectures. \n",
    "            But a fun teacher who's excited to share English with students, normal amount of homework assignments and a small 5 page paper due at finals. 8/10\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good teacher took second block class accident',\n",
       " 'lot reading class super necessary lecture',\n",
       " 'fun teacher excited share english student normal amount homework assignment small page paper due final',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning text\n",
    "wordnet = WordNetLemmatizer()           # can use stemming if desired, but for sentiment analysis i think lemmatizer makes more sense\n",
    "corpus = []\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])         # remove every non-letter character\n",
    "    review = review.lower()                                 # make entirely lowercase\n",
    "    review = review.split()\n",
    "\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words(\"english\"))]        # remove stopwords\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer         # ooh looky, it's scikitlearn\n",
    "\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray()          # create matrix (numpy array) of word occurences by sentence\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8ce2715666cca47edfe3dc40f16f58fc193b5a7b06958e2ae06c97afdf9990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
